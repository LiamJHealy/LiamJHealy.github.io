I"ñ<p>Usually, all graph optimization papers or tutorials start from raising the optimization problem: minimizing the following function:</p>

\[F(x)=\sum_{ij}{e_{ij}(x)^T\Omega_{ij}e_{ij}(x)}\]

<p>This article will explain where this optimization comes from and why it is related to Gaussian noise.</p>

<h2 id="maximum-likelihood-estimation-with-gaussian-noise">Maximum Likelihood Estimation with Gaussian noise</h2>

<p>The probabilistic modeling of graph optimization is based on the Maximum Likelihood Estimation(MLE) algorithm. (More information on Chapter 6 of Xiang Gao‚Äôs book.).</p>

<p>The case we use for this article will also be the one we used in the last article:</p>

<p><img src="\images\SLAM\g2o\g2o_edge_vertex_noedge.png" alt="g2o_edge_vertex_noedge" /></p>

<!-- more -->

<p>What we want to estimate are:</p>

<ul>
  <li>Camera pose \(X_i\)</li>
  <li>Camera pose \(X_j\)</li>
  <li>3D Landmark \(Z_{ij}\)</li>
</ul>

<p>The 3D landmark can be seen by the camera at pose \(i\) and \(j\), the projections of the landmark on both cameras are our measurements:</p>

<ul>
  <li>
    <p>\(x_{ij}\) : 2D projection of \(Z_{ij}\) on camera at pose \(i\)</p>
  </li>
  <li>
    <p>\(x'_{ij}\) : 2D projection of \(Z_{ij}\) on camera at pose \(j\)</p>
  </li>
</ul>

<p><strong>The basic idea of MLE is looking for the \(X_i,X_j\) and \(Z_{ij}\) to maximize the probability of getting the measurements \(x_{ij}\) and \(x'_{ij}\).</strong></p>

<p>This probability is the so-called ‚Äúlikelihood‚Äù. Assuming that the noise we get in the measurement process is Gaussian noise, the probability can be modeled using Gaussian distribution.</p>

<p>Let‚Äôs say the projection of \(Z_{ij}\) onto the camera at pose \(X_i\) is a function \(Proj\):</p>

\[\hat x_{ij}=Proj(Z_{ij},X_i)\]

<p>The \(\hat x_{ij}\) is then the estimated measurement. The probability of getting the measured data \(x_{ij}\) is expressed as the likelihood between this estimation and the real measurement:</p>

\[\mathcal{L}(x_{ij},\hat x_{ij})=\frac{1}{2\sigma_{ij}^2}e^{\displaystyle  -\frac{(x_{ij}-\hat x_{ij})^2}{2\sigma_{ij}^2}}\]

<p><strong>Where \(\sigma_{ij}\) is considered the noise of our measurement.</strong></p>

<p>Combining all the measurements together, we get:</p>

\[\mathcal{L}=\prod{\mathcal{L}(x_{ij},\hat x_{ij})}=\prod{\mathcal{L}(x_{ij},Proj(Z_{ij},X_i))}\]

<p>The problem turns to be maximize the above likelihood function:</p>

\[\hat Z,\hat X= \displaystyle \arg \max_{X,Z}{\prod{\mathcal{L}(x_{ij},Proj(Z_{ij},X_i))}}\]

<p>In order to get rid of the exponential root \(e\) and turn this maximization problem to be a minimization problem, we minimize the -log likelihood instead:</p>

\[\hat Z,\hat X= \displaystyle \arg \min_{X,Z}{\{-log[\prod{\mathcal{L}(x_{ij},Proj(Z_{ij},X_i))]\}}}\]

<p>then we get:</p>

\[\hat Z,\hat X= \displaystyle \arg \min_{X,Z}{\{-\sum{log[\mathcal{L}(x_{ij},Proj(Z_{ij},X_i))]\}}}\]

<p>After some calculation, we get:</p>

\[\hat Z,\hat X= \displaystyle \arg \min_{X,Z}{\sum{\frac{1}{\sigma_{ij}^2}}(x_{ij}-Proj(Z_{ij},X_i))^2}\]

<p>In the graph optimization convention, we define a new term:</p>

\[\displaystyle \Omega_{ij}=\frac{1}{\sigma_{ij}^2},\]

<p><strong>this is the so-called ‚Äúinformation‚Äù matrix. This information matrix  \(\Omega_{ij}\) is a diagonal matrix, as usually the different dimensions of the measurement are independent.</strong></p>

<p>The difference of real measurement and the  estimated measurement is expressed as an error :</p>

\[e_{ij}(x)=x_{ij}-Proj(Z_{ij},X_i),\]

<p>where the new \(x\) represents all unknowns including \(X_i\) and \(Z_{ij}\).</p>

<p>\(e_{ij}(x)\) is taking care of the error related to \(X_i\) and  \(Z_{ij}\) . For bundle adjustment, there is another error between \(X_j\) and \(Z_{ij}\):</p>

\[e'_{ij}(x)=x'_{ij}-Proj(Z_{ij},X_j),\]

<p>and there is also a:</p>

\[\displaystyle \Omega'_{ij}=\frac{1}{\sigma _{ij}^{'2}},\]

<p>We only use \(e_{ij}\) and \(\Omega_{ij}\) to represent both sides, just for convenience.</p>

<p>Don‚Äôt forget that all these parameters have many degrees of freedom, so they are actually vectors. Then our new \(x\) is also a vector. The negative log-likelihood turns to be:</p>

\[F(x)=-log(\mathcal{L})=\sum_{ij}{e_{ij}(x)^T\Omega_{ij}e_{ij}(x)}\]

<p>Now we converge to the convention of graph optimization, usually where every graph optimization starts from setting the optimization goal of minimizing:</p>

\[F(x)=\sum_{ij}{e_{ij}(x)^T\Omega_{ij}e_{ij}(x)}\]
:ET